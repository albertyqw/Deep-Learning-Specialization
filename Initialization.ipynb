{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# in the past we've simply used random initialization\n",
    "# different techniques can be used to better your training\n",
    "# an appropriate initialization will:\n",
    "# speed up the convergence of gradient descent (round bowl)\n",
    "# or lower training error\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from init_utils import sigmoid, relu, compute_loss, forward_propagation, backward_propagation\n",
    "from init_utils import update_parameters, predict, load_dataset, plot_decision_boundary, predict_dec\n",
    "\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (7.0, 4.0) # set default size of plots\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "\n",
    "# load image dataset: blue/red dots in circles\n",
    "train_X, train_Y, test_X, test_Y = load_dataset()\n",
    "\n",
    "# we'll be using a classifier to sperate the red and blue dots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a 3-layer NN has allready be implemented\n",
    "# we'll fiddle around with three initialization methods:\n",
    "# ZEROS - initialzation = \"zeros\" as argument\n",
    "# RANDOM - '''random'''\n",
    "# HE - '''he''' (initalizes weights to scaled random values (He et al., 2015)\n",
    "\n",
    "def model(X, Y, learning_rate = 0.01, num_iterations = 15000, print_cost = True, initialization = \"he\"):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
    "    learning_rate -- learning rate for gradient descent \n",
    "    num_iterations -- number of iterations to run gradient descent\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the loss\n",
    "    m = X.shape[1] # number of examples\n",
    "    layers_dims = [X.shape[0], 10, 5, 1]\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    if initialization == \"zeros\":\n",
    "        parameters = initialize_parameters_zeros(layers_dims)\n",
    "    elif initialization == \"random\":\n",
    "        parameters = initialize_parameters_random(layers_dims)\n",
    "    elif initialization == \"he\":\n",
    "        parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the loss\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per hundreds)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zero initialization of weight matrices and bias vectors\n",
    "# the following function fails to \"break symmetry\"\n",
    "def initialize_parameters_zeros(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # number of layers in the network\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.zeros((layers_dims[l], layers_dims[l - 1]))\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        # note shapes\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_zeros([3,2,1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = model(train_X, train_Y, initialization = \"zeros\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)\n",
    "\n",
    "# trains the model on 15,000 iterations (using zeros initialization)\n",
    "# we get a train and test set accuracies of 0.5, with a minimal to none cost reduction\n",
    "print (\"predictions_train = \" + str(predictions_train))\n",
    "print (\"predictions_test = \" + str(predictions_test))\n",
    "\n",
    "plt.title(\"Model with Zeros initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n",
    "# the reason why this happens is because the algorithm is guessing 0 for every dot\n",
    "# failing to \"break symmetry\" means that every neuron is learning the same thing\n",
    "# you might as well use n[l] = 1 for every layer\n",
    "# takeaway: weights must be initalized randomly to break symmetry, biases can be initialzied as zero however"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters_random(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)               # This seed makes sure your \"random\" numbers will be the same as ours\n",
    "    parameters = {}\n",
    "    L = len(layers_dims)            # integer representing the number of layers\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        ### START CODE HERE ### (â‰ˆ 2 lines of code)\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * 10\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_random([3, 2, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "# note that we're using large (*10) randomized values\n",
    "parameters = model(train_X, train_Y, initialization = \"random\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)\n",
    "\n",
    "# train set: 83, test set: 86\n",
    "# don't worry about the inf cost, this is due to numerical roundoff\n",
    "\n",
    "print (predictions_train)\n",
    "print (predictions_test)\n",
    "# the model is no longer spitting out only zeros\n",
    "\n",
    "plt.title(\"Model with large random initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n",
    "\n",
    "# takeaways: large randomized values result in high starting costs (given sigmoid characteristics)\n",
    "# this might also lead to vanishing / exploding gradients\n",
    "# let's try smaller randomized values, but how much smaller is the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is \"He initialization\" (maybe give He et al., 2015 a read)\n",
    "# similar to \"Xavier initialization\", except a small difference in the scaling factor\n",
    "\n",
    "def initialize_parameters_he(layers_dims):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    layer_dims -- python array (list) containing the size of each layer.\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    W1 -- weight matrix of shape (layers_dims[1], layers_dims[0])\n",
    "                    b1 -- bias vector of shape (layers_dims[1], 1)\n",
    "                    ...\n",
    "                    WL -- weight matrix of shape (layers_dims[L], layers_dims[L-1])\n",
    "                    bL -- bias vector of shape (layers_dims[L], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    parameters = {}\n",
    "    L = len(layers_dims) - 1 # integer representing the number of layers\n",
    "     \n",
    "    for l in range(1, L + 1):\n",
    "        parameters['W' + str(l)] = np.random.randn(layers_dims[l], layers_dims[l-1]) * ((2 / layers_dims[l - 1]) ** 0.5)\n",
    "        parameters['b' + str(l)] = np.zeros((layers_dims[l], 1))\n",
    "        # note math.sqrt wasn't imported\n",
    "        \n",
    "    return parameters\n",
    "\n",
    "parameters = initialize_parameters_he([2, 4, 1])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))\n",
    "\n",
    "parameters = model(train_X, train_Y, initialization = \"he\")\n",
    "print (\"On the train set:\")\n",
    "predictions_train = predict(train_X, train_Y, parameters)\n",
    "print (\"On the test set:\")\n",
    "predictions_test = predict(test_X, test_Y, parameters)\n",
    "# training on 15,000 iterations\n",
    "# train set: 99.333333, test set: 96\n",
    "\n",
    "plt.title(\"Model with He initialization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,1.5])\n",
    "axes.set_ylim([-1.5,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)\n",
    "# :O the plot is practically perfect!\n",
    "\n",
    "# summary: Model\tTrain accuracy\tProblem/Comment\n",
    "# 3-layer NN with zeros initialization\t50%\tfails to break symmetry\n",
    "# 3-layer NN with large random initialization\t83%\ttoo large weights\n",
    "# 3-layer NN with He initialization\t99%\trecommended method\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
